\begin{abstract}
    Partitioning of state spaces is used in Reinforcement Learning when a
    continuous state space has to be explored by a as classical discrete
    Q-learning algorithm or by methods for synthesis of safety shields. Such
    state space partitionings can easily become overwhelmingly large if they
    are to capture an appropriate amount of detail to solve the problem.
    Unfortunately, a large size is a detriment to both explainability,
    verifiability and space and time efficiency of learning. In this work, we
    propose MaxPartitions, a novel and lossless minimization algorithm.
    MaxPartitions represents partitionings as decision trees and performs
    minimization while preserving an equivalent state-action mapping. We show
    that MaxPartitions is able to produce substantial size reductions on both
    controller strategies and safety shields on a number of problems known from
    control theory. We compare MaxPartitions with VIPER, the state-of-the-art
    algorithm for producing small decision trees for reinforcement learning
    strategies, to show, that the loss-less nature of MaxPartitions makes it
    preferable when safety is a requirement.
\end{abstract}
